{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\rushiil bhatnagar\\anaconda3\\envs\\videoanalysispipeline\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rushiil bhatnagar\\anaconda3\\envs\\videoanalysispipeline\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rushiil bhatnagar\\anaconda3\\envs\\videoanalysispipeline\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rushiil bhatnagar\\anaconda3\\envs\\videoanalysispipeline\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rushiil bhatnagar\\anaconda3\\envs\\videoanalysispipeline\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't resolve requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 220\u001b[0m\n\u001b[0;32m    217\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[43mcapture_and_process_grasp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m, in \u001b[0;36mcapture_and_process_grasp\u001b[1;34m(target_classes, grasp_object)\u001b[0m\n\u001b[0;32m     55\u001b[0m config\u001b[38;5;241m.\u001b[39menable_stream(rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mdepth, \u001b[38;5;241m1280\u001b[39m, \u001b[38;5;241m720\u001b[39m, rs\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;241m.\u001b[39mz16, \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Start streaming\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m profile \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m align \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39malign(rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mcolor)\n\u001b[0;32m     60\u001b[0m depth_profile \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mvideo_stream_profile(profile\u001b[38;5;241m.\u001b[39mget_stream(rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mdepth))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't resolve requests"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "from datetime import datetime\n",
    "from utils import get_real_world_coordinates, transform_coordinates, create_zip_archive, process_images\n",
    "\n",
    "def calculate_2d_angle(center_point, grasp_point):\n",
    "    \"\"\"Calculate 2D angle in degrees between grasp direction and positive x-axis.\"\"\"\n",
    "    delta_x = grasp_point[0] - center_point[0]\n",
    "    delta_y = center_point[1] - grasp_point[1]  # Inverted because y increases downward\n",
    "    angle_rad = np.arctan2(delta_y, delta_x)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    return (angle_deg + 360) % 360\n",
    "\n",
    "def calculate_3d_angle(center_xyz, grasp_xyz):\n",
    "    \"\"\"Calculate 3D angles (theta and phi) in spherical coordinates.\"\"\"\n",
    "    rel_pos = grasp_xyz - center_xyz\n",
    "    r = np.linalg.norm(rel_pos)\n",
    "    \n",
    "    # Calculate theta (azimuthal angle in x-y plane from x-axis)\n",
    "    theta = np.degrees(np.arctan2(rel_pos[1], rel_pos[0]))\n",
    "    theta = (theta + 360) % 360\n",
    "    \n",
    "    # Calculate phi (polar angle from z-axis)\n",
    "    phi = np.degrees(np.arccos(rel_pos[2] / r))\n",
    "    \n",
    "    return theta, phi, r\n",
    "\n",
    "def determine_pickup_mode(angle_2d):\n",
    "    \"\"\"Determine pickup mode based on 2D angle.\"\"\"\n",
    "    normalized_angle = angle_2d if angle_2d <= 180 else angle_2d - 180\n",
    "    threshold = 30\n",
    "    \n",
    "    if (normalized_angle <= threshold or normalized_angle >= 180 - threshold):\n",
    "        return \"HORIZONTAL PICKUP\"\n",
    "    elif (90 - threshold <= normalized_angle <= 90 + threshold):\n",
    "        return \"VERTICAL PICKUP\"\n",
    "    else:\n",
    "        return \"UNDEFINED\"\n",
    "\n",
    "def remap_angle(theta, in_min=250, in_max=150, out_min=30, out_max=125):\n",
    "    \"\"\"Remap theta angle to robot angle range.\"\"\"\n",
    "    robot_angle = (theta - in_min) * (out_max - out_min) / (in_max - in_min) + out_min\n",
    "    return 180 - robot_angle\n",
    "\n",
    "def capture_and_process_grasp(target_classes=['bottle'], grasp_object='bottle'):\n",
    "    # Initialize RealSense\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "    config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)\n",
    "    \n",
    "    # Start streaming\n",
    "    profile = pipeline.start(config)\n",
    "    align = rs.align(rs.stream.color)\n",
    "    depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "    depth_intrinsics = depth_profile.get_intrinsics()\n",
    "\n",
    "    # Initialize YOLO for object detection\n",
    "    yolo_model = YOLO('yolov8l-world.pt')\n",
    "    yolo_model.set_classes(target_classes)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Show preview frames\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            color_frame = frames.get_color_frame()\n",
    "            if not color_frame:\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy array for display\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            cv2.imshow('Press \"s\" to capture, \"q\" to quit', color_image)\n",
    "            \n",
    "            key = cv2.waitKey(1)\n",
    "            if key == ord('s'):\n",
    "                # Get aligned frames\n",
    "                frames = pipeline.wait_for_frames()\n",
    "                aligned_frames = align.process(frames)\n",
    "                depth_frame = aligned_frames.get_depth_frame()\n",
    "                color_frame = aligned_frames.get_color_frame()\n",
    "                \n",
    "                if not depth_frame or not color_frame:\n",
    "                    print(\"Failed to capture frames\")\n",
    "                    continue\n",
    "\n",
    "                # Create directories and save frames\n",
    "                root_dir = \"captured_frames\"\n",
    "                current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                folder_name = f\"{root_dir}/captured_frame_{current_time}\"\n",
    "                \n",
    "                os.makedirs(f'{folder_name}/rgb_image', exist_ok=True)\n",
    "                os.makedirs(f'{folder_name}/depth_image', exist_ok=True)\n",
    "                os.makedirs(f'{folder_name}/output', exist_ok=True)\n",
    "\n",
    "                color_image = np.asanyarray(color_frame.get_data())\n",
    "                depth_image = np.asanyarray(depth_frame.get_data())\n",
    "                \n",
    "                cv2.imwrite(f'{folder_name}/rgb_image/image_0.jpg', color_image)\n",
    "                np.save(f'{folder_name}/depth_image/image_0.npy', depth_image)\n",
    "\n",
    "                # Create zip archives\n",
    "                rgb_zip_path = f\"{folder_name}/rgb.zip\"\n",
    "                depth_zip_path = f\"{folder_name}/depth.zip\"\n",
    "                create_zip_archive(f'{folder_name}/rgb_image', rgb_zip_path)\n",
    "                create_zip_archive(f'{folder_name}/depth_image', depth_zip_path)\n",
    "\n",
    "                # Process frames for grasp detection\n",
    "                decoded_csv_data, keypoints = process_images(rgb_zip_path, depth_zip_path, output_dir=f\"{folder_name}/output\")\n",
    "                \n",
    "                # Detect object\n",
    "                results = yolo_model(color_image)\n",
    "                object_center = None\n",
    "                for result in results[0].boxes.data:\n",
    "                    x1, y1, x2, y2, conf, class_id = result\n",
    "                    if yolo_model.names[int(class_id)] == grasp_object:\n",
    "                        center_x = int((x1 + x2) / 2)\n",
    "                        center_y = int((y1 + y2) / 2)\n",
    "                        object_center = (center_x, center_y)\n",
    "                        break\n",
    "\n",
    "                if keypoints and len(keypoints) > 0 and object_center:\n",
    "                    keypoint_3, keypoint_7 = keypoints[0][3], keypoints[0][7]\n",
    "                    \n",
    "                    # Get real world coordinates\n",
    "                    real_world_point_3 = get_real_world_coordinates(folder_name, keypoint_3[0], keypoint_3[1])\n",
    "                    real_world_point_7 = get_real_world_coordinates(folder_name, keypoint_7[0], keypoint_7[1])\n",
    "                    real_world_object = get_real_world_coordinates(folder_name, object_center[0], object_center[1])\n",
    "                    \n",
    "                    # Transform coordinates\n",
    "                    transformed_point_3 = transform_coordinates(*real_world_point_3)\n",
    "                    transformed_point_7 = transform_coordinates(*real_world_point_7)\n",
    "                    transformed_object = transform_coordinates(*real_world_object)\n",
    "\n",
    "                    # Calculate grasp center\n",
    "                    center_x = (keypoint_3[0] + keypoint_7[0]) // 2\n",
    "                    center_y = (keypoint_3[1] + keypoint_7[1]) // 2\n",
    "                    grasp_center = (center_x, center_y)\n",
    "                    \n",
    "                    # Get real world coordinates for grasp center\n",
    "                    real_world_grasp = get_real_world_coordinates(folder_name, center_x, center_y)\n",
    "                    transformed_grasp = transform_coordinates(*real_world_grasp)\n",
    "\n",
    "                    # Calculate angles\n",
    "                    angle_2d = calculate_2d_angle(object_center, grasp_center)\n",
    "                    theta, phi, radius = calculate_3d_angle(\n",
    "                        np.array(transformed_object), \n",
    "                        np.array(transformed_grasp)\n",
    "                    )\n",
    "                    robot_angle = remap_angle(theta)\n",
    "                    pickup_mode = determine_pickup_mode(angle_2d)\n",
    "\n",
    "                    # Visualize results\n",
    "                    vis_img = color_image.copy()\n",
    "                    \n",
    "                    # Draw keypoints and grasp line\n",
    "                    cv2.circle(vis_img, (int(keypoint_3[0]), int(keypoint_3[1])), 5, (0, 0, 255), -1)  # Red\n",
    "                    cv2.circle(vis_img, (int(keypoint_7[0]), int(keypoint_7[1])), 5, (255, 0, 0), -1)  # Blue\n",
    "                    cv2.line(vis_img, (int(keypoint_3[0]), int(keypoint_3[1])), \n",
    "                            (int(keypoint_7[0]), int(keypoint_7[1])), (0, 255, 0), 2)  # Green line\n",
    "                    \n",
    "                    # Draw object center and grasp center\n",
    "                    cv2.circle(vis_img, object_center, 5, (255, 255, 0), -1)  # Cyan\n",
    "                    cv2.circle(vis_img, grasp_center, 5, (0, 255, 255), -1)  # Yellow\n",
    "\n",
    "                    # Add text for angles and modes\n",
    "                    cv2.putText(vis_img, f\"2D Angle: {angle_2d:.2f}°\", (10, 30), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                    cv2.putText(vis_img, f\"3D Theta: {theta:.2f}°\", (10, 60), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                    cv2.putText(vis_img, f\"Robot Angle: {robot_angle:.2f}°\", (10, 90), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                    cv2.putText(vis_img, f\"Mode: {pickup_mode}\", (10, 120), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "                    # Save results to JSON\n",
    "                    results_data = {\n",
    "                        'timestamp': current_time,\n",
    "                        'angles': {\n",
    "                            '2d_angle': float(angle_2d),\n",
    "                            '3d_theta': float(theta),\n",
    "                            '3d_phi': float(phi),\n",
    "                            'robot_angle': float(robot_angle)\n",
    "                        },\n",
    "                        'pickup_mode': pickup_mode,\n",
    "                        'keypoints': {\n",
    "                            'point_3': transformed_point_3.tolist(),\n",
    "                            'point_7': transformed_point_7.tolist(),\n",
    "                            'grasp_center': transformed_grasp.tolist(),\n",
    "                            'object_center': transformed_object.tolist()\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    import json\n",
    "                    with open(f'{folder_name}/output/grasp_analysis.json', 'w') as f:\n",
    "                        json.dump(results_data, f, indent=4)\n",
    "\n",
    "                    cv2.imshow('Grasp Analysis', vis_img)\n",
    "                    cv2.waitKey(0)\n",
    "\n",
    "                    # Save visualization\n",
    "                    cv2.imwrite(f'{folder_name}/output/analyzed_grasp.jpg', vis_img)\n",
    "                    print(f\"Results saved in {folder_name}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"No keypoints or object detected\")\n",
    "\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        pipeline.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    capture_and_process_grasp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VideoAnalysisPipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
